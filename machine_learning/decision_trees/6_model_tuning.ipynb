{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "- **Problem**: search for a set of optimal hyperparameters for a learning algorithm.\n",
    "- **Solution**: find a set of optimal hyperparameters that results in an optimal model. \n",
    "- **Optimal model**: yields an optimal **score**. \n",
    "- **Score**: in sklearn defaults to accuracy (classification) and R2 (regression).\n",
    "- **Important**: a model generalization performance is tested using cross-validation.\n",
    "\n",
    "### Approaches\n",
    "- **Grid Search**: \n",
    "    - Manually set a grid of discrete hyperparameter values.\n",
    "    - Set a metric for scoring model performance.\n",
    "    - Search exhaustively through the grid. \n",
    "    - For each set of hyperparameters, evaluate each model's CV score.\n",
    "    - The optimal hyperparameters are those of the model achieving the best CV score. \n",
    "\n",
    "- **Random Search**:\n",
    "\n",
    "- **Bayesian Optimization**:\n",
    "\n",
    "- **Genetic Algorithms**:\n",
    "\n",
    "\n",
    "\n",
    "### Definitions\n",
    "- **paramaters**: learned from data\n",
    "    - Example: split-point of a node, split-feature of a node.. \n",
    "- **hyperparameters**: not learned from data, set prior to training\n",
    "    - Example: max_depth, min_samples_leaf, splitting criterion...\n",
    "\n",
    "\n",
    "### Cons\n",
    "- Tuning is expensive\n",
    "    - computationally expensive\n",
    "    - sometimes leads to very slight improvements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# Instantiate the model\n",
    "dt = DecisionTreeClassifier(random_state=seed)\n",
    "print(dt.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams:\n",
      " {'max_depth': 3, 'max_features': 0.2, 'min_samples_leaf': 0.04}\n",
      "Best CV accuracy: \n",
      " 0.9373076923076923\n",
      "Test set accuracy of best model: \n",
      " 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data.data[:, :]\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "params_dt = {\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"min_samples_leaf\": [0.04, 0.06, 0.08],\n",
    "    \"max_features\": [0.2, 0.4, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "# Instantiate a grid search CV object\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='accuracy',\n",
    "                       cv=10,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Fit to the training data\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print(f\"Best hyperparams:\\n {best_hyperparams}\")\n",
    "\n",
    "# Extract best CV score\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print(f\"Best CV accuracy: \\n {best_CV_score}\")\n",
    "\n",
    "# Extract the best model from 'grid_dt'\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Evaluate test set accuracy \n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print(f\"Test set accuracy of best model: \\n {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphrodite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

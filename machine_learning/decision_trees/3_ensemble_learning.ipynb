{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "**Definition**:\n",
    "Is a machine learning technique that combines several base models in order to produce one optimal predictive model. \n",
    "- Final prediction: more robust and less prone to erros.\n",
    "- Best results: models are skillful in different ways.\n",
    "\n",
    "**Types**:\n",
    "- **Voting**:\n",
    "    - Train different models on the same dataset.\n",
    "    - Let each model make its predictions. \n",
    "    - Meta-model: aggregates predictions of individual models by majority voting. \n",
    "- **Bagging**:\n",
    "    - Base estimator: Decision Tree, Logistic Regression, Neural Net, etc... \n",
    "    - Each estimator is trained on a distinct bootstrap sample of the training set\n",
    "    - Estimators use all features for training and prediction\n",
    "- Boosting\n",
    "\n",
    "![title](https://drive.google.com/uc?export=view&id=1B4gM0ChrAk3C41BSjU4QasfSkFLzwAfk)\n",
    "\n",
    "### Concepts:\n",
    "**Out-of-bag evaluation**: Most random forestes use a technique called out-of-bag evaluation (OOB evaluation) to evaluate the quality of the model. OOB treats the training set as if it were on the test set of cross-validation. \n",
    "- Each decision tree in a random forest is typically trained on ~67% of the training examples. Therefore, each decision tree does not see ~33% of the training examples. \n",
    "- It helps us understand how well the model performs on unseen data, without cross-validation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.8713450292397661\n",
      "Classification Tree: 0.8128654970760234\n",
      "K Nearest Neighbors: 0.8596491228070176\n",
      "Voting Classifier: 0.8713450292397661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data.data[:, :2]\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "# Load classifiers\n",
    "lr = LogisticRegression(random_state=seed)\n",
    "dt = DecisionTreeClassifier(random_state=seed)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define a list with classifiers\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "               ('Classification Tree', dt),\n",
    "               ('K Nearest Neighbors', knn)]\n",
    "\n",
    "# Evaluate models\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{clf_name}: {score}\")\n",
    "\n",
    "# Voting classifier\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree Classifier is: 0.8771929824561403\n",
      "The accuracy of the Bagging Classifier is: 0.8830409356725146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=seed)\n",
    "\n",
    "# Instantiate models\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=seed)\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Train\n",
    "dt.fit(X_train, y_train)\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Precit\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"The accuracy of the Decision Tree Classifier is: {accuracy_dt}\")\n",
    "print(f\"The accuracy of the Bagging Classifier is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphrodite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
